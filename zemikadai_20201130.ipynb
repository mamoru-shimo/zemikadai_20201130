{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNISTのデータを扱うための前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    #MNISTデータをpathからロード。引数に指定したパスを結合（ラベルや画像のパスを作成）\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    \n",
    "    #ファイルを読み込む\n",
    "    #引数にファイル、モードを指定　rb（read binary）は読み込みのバイナリモード）\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        #バイナリを文字列に変換：unpack関数の引数にフォーマット、8バイト分の\n",
    "        #バイナリデータを指定してマジックナンバー、アイテムの個数を読み込む\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        #ファイルからラベルを読み込み配列を構築：fromfile関数の引数に\n",
    "        #ファイル、配列のデータ形式を指定\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    \n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols =struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        #画像ピクセル情報の配列のサイズを変更\n",
    "        #（行数：ラベルのサイズ、数列：特徴量の個数）\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "        images = ((images / 255.) - .5) * 2\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, Columns: 784\n",
      "Rows: 10000, Columns: 784\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_mnist('.', kind='train')\n",
    "print('Rows: %d, Columns: %d' %(x_train.shape[0],x_train.shape[1]))\n",
    "x_test, y_test = load_mnist('.' ,kind='t10k')\n",
    "print('Rows: %d, Columns: %d' %(x_test.shape[0],x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#標準化、正規化\n",
    "mean_vals = np.mean(x_train, axis=0)\n",
    "std_val = np.std(x_train)\n",
    "\n",
    "x_train_cntered = (x_train - mean_vals) / std_val\n",
    "x_test_centered = (x_test - mean_vals) / std_val\n",
    "del x_train, x_test\n",
    "print(x_train_cntered.shape, y_train.shape)\n",
    "print(x_test_centered.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~Tensorflow~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算グラフの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b1d47a80b73e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     tf_x = tf.placeholder(dtype=tf.float32,\n\u001b[0m\u001b[0;32m     13\u001b[0m                        \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                        name='tf_x')\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_features = x_train_cntered.shape[1]\n",
    "n_classes = 10\n",
    "random_seed =123\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "#計算グラフの作成開始\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    tf.random.set_seed(random_seed)\n",
    "    tf_x = tf.placeholder(dtype=tf.float32,\n",
    "                       shape=(None, n_features),\n",
    "                       name='tf_x')\n",
    "\n",
    "    tf_y = tf.placeholder(dtype=tf.int32, \n",
    "                        shape=None, name='tf_y')\n",
    "    y_onehot = tf.one_hot(indices=tf_y, depth=n_classes)\n",
    "\n",
    "    h1 = tf.layers.dense(inputs=tf_x, units=50,\n",
    "                         activation=tf.tanh,\n",
    "                         name='layer1')\n",
    "\n",
    "    h2 = tf.layers.dense(inputs=h1, units=50,\n",
    "                         activation=tf.tanh,\n",
    "                         name='layer2')\n",
    "\n",
    "    logits = tf.layers.dense(inputs=h2, \n",
    "                             units=10,\n",
    "                             activation=None,\n",
    "                             name='layer3')\n",
    "\n",
    "    predictions = {\n",
    "        'classes' : tf.argmax(logits, axis=1, \n",
    "                              name='predicted_classes'),\n",
    "        'probabilities' : tf.nn.softmax(logits, \n",
    "                              name='softmax_tensor')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.losses' has no attribute 'softmax_cross_entropy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1134be9734ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#コスト関数とオプティマイザを定義\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     cost = tf.losses.softmax_cross_entropy(\n\u001b[0m\u001b[0;32m      4\u001b[0m             onehot_labels=y_onehot, logits=logits)\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.keras.losses' has no attribute 'softmax_cross_entropy'"
     ]
    }
   ],
   "source": [
    "#コスト関数とオプティマイザを定義\n",
    "with g.as_default():\n",
    "    cost = tf.losses.softmax_cross_entropy(\n",
    "            onehot_labels=y_onehot, logits=logits)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=0.001)\n",
    "\n",
    "    train_op = optimizer.minimize(loss=cost)\n",
    "#モデルの変数とオプティマイザを初期化するための演算子を定義\n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データバッチ用のジェネレータを返す関数を実装\n",
    "def create_batch_generator(X, y, batch_size=128, shuffle=False):\n",
    "    X_copy = np.array(X)\n",
    "    y_copy = np.array(y)\n",
    "    \n",
    "    if shuffle:\n",
    "        data = np.column_stack((X_copy, y_copy))\n",
    "        np.random.shuffle(data)\n",
    "        X_copy = data[:, :-1]\n",
    "        y_copy = data[:, -1].astype(int)\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        yield (X_copy[i:i+batch_size, :], y_copy[i:i+batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# セッションを開始して実際に計算グラフを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-010bdf8cf411>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## 計算グラフを起動するためのセッションを作成\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m## 変数イニシャライザを実行\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "## 計算グラフを起動するためのセッションを作成\n",
    "sess =  tf.Session(graph=g)\n",
    "## 変数イニシャライザを実行\n",
    "sess.run(init_op)\n",
    "\n",
    "## 50 エッポクのトレーニング\n",
    "training_costs = []\n",
    "for epoch in range(50):\n",
    "    training_loss = []\n",
    "    batch_generator = create_batch_generator(\n",
    "            X_train_centered, y_train, \n",
    "            batch_size=64)\n",
    "    for batch_X, batch_y in batch_generator:\n",
    "        ## prepare a dict to feed data to our network:\n",
    "        feed = {tf_x:batch_X, tf_y:batch_y}\n",
    "        _, batch_cost = sess.run([train_op, cost],\n",
    "                                 feed_dict=feed)\n",
    "        training_costs.append(batch_cost)\n",
    "    print(' -- Epoch %2d  '\n",
    "          'Avg. Training Loss: %.4f' % (\n",
    "              epoch+1, np.mean(training_costs)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 上記でトレーニング済みのモデルをテストデータセットで使用して予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9c3c3b43a062>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_x\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mx_test_centered\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classes'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m print('Test Accuracy: %2f%%' %\n\u001b[0;32m      5\u001b[0m      (100 * np.sum(y_pred ==y_test) / y_test.shape[0]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf_x' is not defined"
     ]
    }
   ],
   "source": [
    "feed = {tf_x : x_test_centered}\n",
    "y_pred = sees.run(predictions['classes'], feed_dict=feed)\n",
    "\n",
    "print('Test Accuracy: %2f%%' %\n",
    "     (100 * np.sum(y_pred ==y_test) / y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
